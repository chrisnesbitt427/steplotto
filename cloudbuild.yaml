steps:


# Deploy the main Cloud Function
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'gcloud'
  args:
  - 'functions'
  - 'deploy'
  - 'insert-to-bigquery'
  - '--gen2'
  - '--runtime=python311'
  - '--trigger-http'
  - '--allow-unauthenticated'
  - '--entry-point=insert_to_bigquery'
  - '--region=europe-west2'
  - '--source=./ingestion'

# Deploy the dummy data generator
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'gcloud'
  args:
  - 'functions'
  - 'deploy'
  - 'generate-dummy-data'
  - '--gen2'
  - '--runtime=python311'
  - '--trigger-topic=dummy-data-topic'
  - '--entry-point=generate_dummy_data'
  - '--region=europe-west2'
  - '--source=./dummy_ingestion'

# Handle scheduler job creation/update
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    # Delete existing job if it exists, then recreate with new schedule
    if gcloud scheduler jobs describe daily-dummy-data --location=europe-west2 2>/dev/null; then
      echo "Deleting existing scheduler job..."
      gcloud scheduler jobs delete daily-dummy-data --location=europe-west2 --quiet
    fi
    echo "Creating scheduler job..."
    gcloud scheduler jobs create pubsub daily-dummy-data \
      --schedule="0 8 * * *" \
      --topic=dummy-data-topic \
      --message-body="{}" \
      --time-zone=Europe/London \
      --location=europe-west2

timeout: '1600s'
options:
  logging: CLOUD_LOGGING_ONLY